{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textgenerationlstm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMKgg/HdMwhTkmtdqi60Ep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrien50/textgeneration/blob/main/textgenerationlstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc-aczhkiHps"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zg4oFKfiI1u"
      },
      "source": [
        "Defining Terms\r\n",
        "\r\n",
        "To begin with, let's start by defining our terms. It may prove difficult to understand why certain lines of code are being executed unless you have a decent understanding of the concepts that are being brought together.\r\n",
        "\r\n",
        "TensorFlow\r\n",
        "\r\n",
        "TensorFlow is one of the most commonly used machine learning libraries in Python, specializing in the creation of deep neural networks. Deep neural networks excel at tasks like image recognition and recognizing patterns in speech. TensorFlow was designed by Google Brain, and its power lies in its ability to join together many different processing nodes.\r\n",
        "\r\n",
        "Keras\r\n",
        "\r\n",
        "Meanwhile, Keras is an application programming interface or API. Keras makes use of TensorFlow's functions and abilities, but it streamlines the implementation of TensorFlow functions, making building a neural network much simpler and easier. Keras' foundational principles are modularity and user-friendliness, meaning that while Keras is quite powerful, it is easy to use and scale.\r\n",
        "\r\n",
        "Natural Language Processing\r\n",
        "\r\n",
        "Natural Language Processing (NLP) is exactly what it sounds like, the techniques used to enable computers to understand natural human language, rather than having to interface with people through programming languages. Natural language processing is necessary for tasks like the classification of word documents or the creation of a chatbot.\r\n",
        "\r\n",
        "Recurrent Neural Network\r\n",
        "\r\n",
        "Recurrent means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements.\r\n",
        "A machine learning model that considers the words in isolation — such as a bag of words model — would probably conclude this sentence is negative. An RNN by contrast should be able to see the words “but” and “terribly exciting” and realize that the sentence turns from negative to positive because it has looked at the entire sequence. Reading a whole sequence gives us a context for processing its meaning, a concept encoded in recurrent neural networks.\r\n",
        "\r\n",
        "Long Short-Term Memory\r\n",
        "\r\n",
        "At the heart of an RNN is a layer made of memory cells. The most popular cell at the moment is the Long Short-Term Memory (LSTM) which maintains a cell state as well as a carry for ensuring that the signal (information in the form of a gradient) is not lost as the sequence is processed. At each time step the LSTM considers the current word, the carry, and the cell state.\r\n",
        "\r\n",
        "\r\n",
        "The LSTM has 3 different gates and weight vectors: there is a “forget” gate for discarding irrelevant information; an “input” gate for handling the current input, and an “output” gate for producing predictions at each time step. However, as Chollet points out, it is fruitless trying to assign specific meanings to each of the elements in the cell.\r\n",
        "\r\n",
        "Text Generation Theory/Approach\r\n",
        "\r\n",
        "One-Hot Encoding\r\n",
        "\r\n",
        "As previously mentioned, there are two main ways of encoding text data. One method is referred to as one-hot encoding, while the other method is called word embedding.\r\n",
        "\r\n",
        "The process of one-hot encoding refers to a method of representing text as a series of ones and zeroes. A vector containing all possible words you are interested in, often all the words in the corpus, is created and a single word is represented by a \"one\" value in its respective position. Meanwhile all other positions (all the other possible words) are given a zero value. A vector like this is created for every word in the feature set, and when the vectors are joined together the result is a matrix containing binary representations of all the feature words.\r\n",
        "\r\n",
        "Here's another way to think about this: any given word is represented by a vector of ones and zeroes, with a one value at a unique position. The vector is essentially concerned with answering the question: \"Is this the target word?\" If the word in the list of feature words is the target a positive value (one) is entered there, and in all other cases the word isn't the target, so a zero is entered. Therefore, you have a vector that represents just the target word. This is done for every word in the list of features.\r\n",
        "\r\n",
        "One-hot encodings are useful when you need to need to create a bag of words, or a representation of words that takes their frequency of occurrence into account. Bag of words models are useful because although they are simple models, they still maintain a lot of important information and are versatile enough to be used for many different NLP related tasks.\r\n",
        "\r\n",
        "One drawback to using one-hot encodings is that they cannot represent the meaning of a word, nor can they easily detect similarities between words. If meaning and similarity are concerns, word embeddings are often used instead.\r\n",
        "\r\n",
        "Word Embeddings\r\n",
        "\r\n",
        "Word embedding refers to representing words or phrases as a vector of real numbers, much like one-hot encoding does. However, a word embedding can use more numbers than simply ones and zeros, and therefore it can form more complex representations. For instance, the vector that represents a word can now be comprised of decimal values like 0.5. These representations can store important information about words, like relationship to other words, their morphology, their context, etc.\r\n",
        "\r\n",
        "Word embeddings have fewer dimensions than one-hot encoded vectors do, which forces the model to represent similar words with similar vectors. Each word vector in a word embedding is a representation in a different dimension of the matrix, and the distance between the vectors can be used to represent their relationship. Word embeddings can generalize because semantically similar words have similar vectors. The word vectors occupy a similar region of the matrix, which helps capture context and semantics.\r\n",
        "\r\n",
        "In general, one-hot vectors are high-dimensional but sparse and simple, while word embeddings are low dimensional but dense and complex.\r\n",
        "\r\n",
        "Word-Level Generation vs Character-Level Generation\r\n",
        "\r\n",
        "There are two ways to tackle a natural language processing task like text generation. You can analyze the data and make predictions about it at the level of the words in the corpus or at the level of the individual characters. Both character-level generation and word-level generation have their advantages and disadvantages.\r\n",
        "\r\n",
        "In general, word-level language models tend to display higher accuracy than character-level language models. This is because they can form shorter representations of sentences and preserve the context between words easier than character-level language models. However, large corpuses are needed to sufficiently train word-level language models, and one-hot encoding isn't very feasible for word level models.\r\n",
        "\r\n",
        "In contrast, character-level language models are often quicker to train, requiring less memory and having faster inference than word-based models. This is because the \"vocabulary\" (the number of training features) for the model is likely to be much smaller overall, limited to some hundreds of characters rather than hundreds of thousands of words.\r\n",
        "\r\n",
        "Character-based models also perform well when translating words between languages because they capture the characters which make up words, rather than trying to capture the semantic qualities of words. We'll be using a character-level model here, in part because of its simplicity and fast inference.\r\n",
        "\r\n",
        "Using an RNN/LSTM\r\n",
        "\r\n",
        "Now we'll be implementing a LSTM and doing text generation with it. First, we'll need to get some text data and preprocess the data. After that, we'll create the LSTM model and train it on the data. Finally, we'll evaluate the network.\r\n",
        "\r\n",
        "For the text generation, we want our model to learn probabilities about what character will come next, when given a starting (random) character. We will then chain these probabilities together to create an output of many characters. We first need to convert our input text to numbers and then train the model on sequences of these numbers.\r\n",
        "\r\n",
        "Let's start out by importing all the libraries we're going to use. We need numpy to transform our input data into arrays our network can use, and we'll obviously be using several functions from Keras.\r\n",
        "\r\n",
        "We'll also need to use some functions from the Natural Language Toolkit (NLTK) to preprocess our text and get it ready to train on. Finally, we'll need the sys library to handle the printing of our text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAimnqkoiLwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3db7e6d-e5e0-4969-8799-34a8395b1992"
      },
      "source": [
        "import numpy\r\n",
        "import sys\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, LSTM\r\n",
        "from keras.utils import np_utils\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwLVsMIqq7jk"
      },
      "source": [
        "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn.\r\n",
        "We'll be training the network on the text from the first 9 chapters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_G6L2-iq8d5"
      },
      "source": [
        "file = open(\"frankenstein-2.txt\").read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRCzHFWArUR4"
      },
      "source": [
        "Let's start by loading in our text data and doing some preprocessing of the data. We're going to need to apply some transformations to the text so everything is standardized and our model can work with it.\r\n",
        "\r\n",
        "We're going to lowercase everything so and not worry about capitalization in this example. We're also going to use NLTK to make tokens out of the words in the input file. Let's create an instance of the tokenizer and use it on our input file.\r\n",
        "\r\n",
        "Finally, we're going to filter our list of tokens and only keep the tokens that aren't in a list of Stop Words, or common words that provide little information about the sentence in question. We'll do this by using lambda to make a quick throwaway function and only assign the words to our variable if they aren't in a list of Stop Words provided by NLTK.\r\n",
        "\r\n",
        "Let's create a function to handle all that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSA7lMQJrVVj"
      },
      "source": [
        "\r\n",
        "def tokenize_words(input):\r\n",
        "    # lowercase everything to standardize it\r\n",
        "    input = input.lower()\r\n",
        "\r\n",
        "    # instantiate the tokenizer\r\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\r\n",
        "    tokens = tokenizer.tokenize(input)\r\n",
        "\r\n",
        "    # if the created token isn't in the stop words, make it part of \"filtered\"\r\n",
        "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\r\n",
        "    return \" \".join(filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSwUDG2_Jzw_"
      },
      "source": [
        "# preprocess the input data, make tokens\r\n",
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrsHnSk4K_GS"
      },
      "source": [
        "A neural network works with numbers, not text characters. So well need to convert the characters in our input to numbers. We'll sort the list of the set of all characters that appear in our input text, then use the enumerate function to get numbers which represent the characters. We then create a dictionary that stores the keys and values, or the characters and the numbers that represent the"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWqkRwJpLJ4Z"
      },
      "source": [
        "chars = sorted(list(set(processed_inputs)))\r\n",
        "char_to_num = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4MWVlGvLX1d"
      },
      "source": [
        "We need the total length of our inputs and total length of our set of characters for later data prep, so we'll store these in a variable. Just so we get an idea of if our process of converting words to characters has worked thus far, let's print the length of our variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x07cVL3RLbum",
        "outputId": "0f64f5a2-902a-4382-bd4e-a358ef921c16"
      },
      "source": [
        "input_len = len(processed_inputs)\r\n",
        "vocab_len = len(chars)\r\n",
        "print (\"Total number of characters:\", input_len)\r\n",
        "print (\"Total vocab:\", vocab_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters: 91612\n",
            "Total vocab: 38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P39H1G1vribw"
      },
      "source": [
        "Now that we've transformed the data into the form it needs to be in, we can begin making a dataset out of it, which we'll feed into our network. We need to define how long we want an individual sequence (one complete mapping of inputs characters as integers) to be. We'll set a length of 100 for now, and declare empty lists to store our input and output data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-ObYPk_LyM4"
      },
      "source": [
        "seq_length = 100\r\n",
        "x_data = []\r\n",
        "y_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inrpLcDnMPuO"
      },
      "source": [
        "Now we need to go through the entire list of inputs and convert the characters to numbers. We'll do this with a for loop. This will create a bunch of sequences where each sequence starts with the next character in the input data, beginning with the first character:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQqb5vwzrh03"
      },
      "source": [
        "# loop through inputs, start at the beginning and go until we hit\r\n",
        "# the final character we can create a sequence out of\r\n",
        "for i in range(0, input_len - seq_length, 1):\r\n",
        "    # Define input and output sequences\r\n",
        "    # Input is the current character plus desired sequence length\r\n",
        "    in_seq = processed_inputs[i:i + seq_length]\r\n",
        "\r\n",
        "    # Out sequence is the initial character plus total sequence length\r\n",
        "    out_seq = processed_inputs[i + seq_length]\r\n",
        "\r\n",
        "    # We now convert list of characters to integers based on\r\n",
        "    # previously and add the values to our lists\r\n",
        "    x_data.append([char_to_num[char] for char in in_seq])\r\n",
        "    y_data.append(char_to_num[out_seq])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LP0a-ERMS6m"
      },
      "source": [
        "Now we have our input sequences of characters and our output, which is the character that should come after the sequence ends. We now have our training data features and labels, stored as x_data and y_data. Let's save our total number of sequences and check to see how many total input sequences we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnyKOtoOMhSF",
        "outputId": "df89eae2-f702-4cb4-8dcf-92aa6b1d473d"
      },
      "source": [
        "n_patterns = len(x_data)\r\n",
        "print (\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns: 91512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIzFdMaet0g8"
      },
      "source": [
        "Now we'll go ahead and convert our input sequences into a processed numpy array that our network can use. We'll also need to convert the numpy array values into floats so that the sigmoid activation function our network uses can interpret them and output probabilities from 0 to 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6dH0dz2t1cX"
      },
      "source": [
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\r\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ew85a7oNCHo"
      },
      "source": [
        "We'll now one-hot encode our label data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4f_KHgNNMYo"
      },
      "source": [
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQK0ydTzNZFX"
      },
      "source": [
        "Since our features and labels are now ready for the network to use, let's go ahead and create our LSTM model. We specify the kind of model we want to make (a sequential one), and then add our first layer.\r\n",
        "We'll do dropout to prevent overfitting, followed by another layer or two. Then we'll add the final layer, a densely connected layer that will output a probability about what the next character in the sequence will be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtJKFNeSNpYr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Fs8ZGaNpwo"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(256, return_sequences=True))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(128))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2Nduj3ONuQ5"
      },
      "source": [
        "We compile the model now, and it is ready for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AZP_-W1Nv2-"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqKMBzuROBt4"
      },
      "source": [
        "It takes the model quite a while to train, and for this reason we'll save the weights and reload them when the training is finished. We'll set a checkpoint to save the weights to, and then make them the callbacks for our future model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSk4PnsEOCUS"
      },
      "source": [
        "filepath = \"model_weights_saved.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ0_WXRYOKbQ"
      },
      "source": [
        "Now we'll fit the model and let it train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X70MPYCnOFsI",
        "outputId": "88ae2488-e33a-441c-91e0-1e5a1f7530a1"
      },
      "source": [
        "model.fit(X, y, epochs=4, batch_size=256, callbacks=desired_callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "358/358 [==============================] - 1372s 4s/step - loss: 3.0094\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.96017, saving model to model_weights_saved.hdf5\n",
            "Epoch 2/4\n",
            "358/358 [==============================] - 1365s 4s/step - loss: 2.8265\n",
            "\n",
            "Epoch 00002: loss improved from 2.96017 to 2.74346, saving model to model_weights_saved.hdf5\n",
            "Epoch 3/4\n",
            "358/358 [==============================] - 1364s 4s/step - loss: 2.5966\n",
            "\n",
            "Epoch 00003: loss improved from 2.74346 to 2.55920, saving model to model_weights_saved.hdf5\n",
            "Epoch 4/4\n",
            "358/358 [==============================] - 1361s 4s/step - loss: 2.4489\n",
            "\n",
            "Epoch 00004: loss improved from 2.55920 to 2.41000, saving model to model_weights_saved.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc7d2c444a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0NTXynllOkz"
      },
      "source": [
        "After it has finished training, we'll specify the file name and load in the weights. Then recompile our model with the saved weights:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhUHG_9NlQ-0"
      },
      "source": [
        "filename = \"model_weights_saved.hdf5\"\r\n",
        "model.load_weights(filename)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm9ExsfBlgZO"
      },
      "source": [
        "Since we converted the characters to numbers earlier, we need to define a dictionary variable that will convert the output of the model back into numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWTmrAfalj7_"
      },
      "source": [
        "num_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m38BXsxl0vQ"
      },
      "source": [
        "To generate characters, we need to provide our trained model with a random seed character that it can generate a sequence of characters from:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s3yu_fbmXHr",
        "outputId": "17225e0d-0792-4d44-b8ee-a12130f0a6b9"
      },
      "source": [
        "start = numpy.random.randint(0, len(x_data) - 1)\r\n",
        "pattern = x_data[start]\r\n",
        "print(\"Random Seed:\")\r\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:\n",
            "\" emed wink one little eyes said nothing perhaps understand english thought alice daresay french mouse \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGoeswVsm6ij"
      },
      "source": [
        "Now to finally generate text, we're going to iterate through our chosen number of characters and convert our input (the random seed) into float values.\r\n",
        "\r\n",
        "We'll ask the model to predict what comes next based off of the random seed, convert the output numbers to characters and then append it to the pattern, which is our list of generated characters plus the initial seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJRvdT60m8uP",
        "outputId": "b5e2d0de-e3c7-42db-b2a3-69dd6c36c64a"
      },
      "source": [
        "for i in range(1000):\r\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\r\n",
        "    x = x / float(vocab_len)\r\n",
        "    prediction = model.predict(x, verbose=0)\r\n",
        "    index = numpy.argmax(prediction)\r\n",
        "    result = num_to_char[index]\r\n",
        "    seq_in = [num_to_char[value] for value in pattern]\r\n",
        "\r\n",
        "    sys.stdout.write(result)\r\n",
        "\r\n",
        "    pattern.append(index)\r\n",
        "    pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said said"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THQVJlEWnWGd"
      },
      "source": [
        "Does this seem somewhat disappointing? Yes, the text that was generated doesn't make any sense, and it seems to start simply repeating patterns after a little bit. However, the longer you train the network the better the text that is generated will be.\r\n",
        "\r\n",
        "For instance, when the number of training epochs was increased to 20, the output looked more like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqWfUaeen6su"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "You'll want to increase the number of training epochs to improve the network's performance. However, you may also want to use either a deeper neural network (add more layers to the network) or a wider network (increase the number of neurons/memory units) in the layers.\r\n",
        "\r\n",
        "You could also try adjusting the batch size, one hot-encoding the inputs, padding the input sequences, or combining any number of these ideas.\r\n",
        "\r\n",
        "If you want to learn more about how LSTMs work, you can read up on the subject here. Learning how the parameters of the model influence the model's performance will help you choose which parameters or hyperparameters to adjust. You may also want to read up on text processing techniques and tools like those provided by NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaFUqsB8nXqq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}